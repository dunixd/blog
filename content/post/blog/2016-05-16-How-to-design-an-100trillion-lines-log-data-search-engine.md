---
categories:
- blog
date: 2016-05-16T20:41:00Z
tags:
- 搜索引擎
- Hadoop
- HDFS
- 倒排索引
title: 如何打造一个百万亿级的日志搜索引擎:Poseidon

---

## 项目背景

安全分析人员经常需要知道一个样本在某个时间段内感染了哪些电脑？范围有多广？源头是从哪里来的？样本家族是什么？
这些信息在海量的病毒查杀日志中都有蛛丝马迹可寻，如是，对海量日志进行快速的分析和检索就成为一种至关重要的能力。
这里海量，不是百亿、万亿，而是百万亿级别(1后面14个零)，占用100PB大小(1后面17个零)。

### 项目需求

1. 总数据量：百万亿条、大小100PB
2. 交互式的搜索响应(秒级)
3. 故障转移、自动恢复、节点负载均衡
4. 自定义分词策略，例如针对路径、URL、PEStringDump等信息能有分别独立自定义的分词策略。

## 现有方案调研

### ElasticSearch

ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。
Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前非常流行的搜索引擎解决方案。

目前使用ES建设的规模比较大的案例是Github，Github使用Elasticsearch搜索20TB的数据，包括13亿的文件和1300亿行的代码，这是之前的数据了，现在应该更大一些。

这里介绍了一些使用ES的案例 [http://www.searchtech.pro/elasticsearch-users-case](http://www.searchtech.pro/elasticsearch-users-case)

以下几点原因决定了ES不适用于当前项目：
1. 是否能够支持百万亿级别的数据规模存在太多不确定因素
2. 数据需要全部倒入到ES系统中，而原始数据由于有Map/Reduce任务也需要存储在Hadoop中，这样导致数据存储上的冗余
3. 自定义分词不灵活，需要定制开发

### HBase

HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。

实现上，HBase是在HDFS之上的KV存储，由于我们的原始日志每一条日志的字段多达100多个，
在测试时发现数据膨胀了7倍多，这根本就是不能接受的。另外，HBase对自定的分词策略支持有限，也需要很多的定制开发。

### 腾讯Hermes

1. 一个基于大索引技术的海量数据实时检索分析平台
1. 侧重数据分析
1. 数据规模从几亿到万亿不等
1. 没有开源
1. 只知道其索引数据也放在HDFS中，没有太多实现细节

## Poseidon详细设计

Poseidon(波塞冬)，是希腊神话中的海神，在这里寓意海量数据的主宰者。

### 设计目标

上述分析了现有的各种方案的一些基本情况，鉴于我们的数据规模实在太大，我们有理由相信全世界能需要如此规模数据集的公司也是屈指可数的，部分需求又很特殊(分词策略)，因此只能自己开发。

，其设计目标如下：

1. 数据总量：百万亿条、大小100PB(17个0)
1. 原始数据不用额外存储
1. 当前的Map/Reduce作业不用变更
1. 自定义分词
1. 秒级响应
1. 模糊匹配

### 搜索引擎核心技术介绍——倒排索引

倒排索引(英语:Inverted index)，也常被称为反向索引、置入档案或反向档案，是一种索引方法，
被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。
它是文档检索系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。

### 在设计之前进行相关数据准备和调研

我们有一个业务的日志数据，每天的新增数据量在700亿条(行)左右，每条大小平均850字节，明文数据量在60TB左右，GZ压缩后占用10TB。

使用一个通用的分词器，对这份日志一天的数据进行分词，得到的数据如下：
1. 总共得到100亿个分词，占用400GB空间
2. 每个分词平均关联277行日志
3. 按倒排索引建库的话，总计需要27700亿个DocId，假如每天DocId占用4字节，需要11TB的空间
4. 上述倒排索引需要进行一定压缩编码，实际应用时争取压缩到2TB

### 基本设计原则

根据上述数据调研，我们每天的增量数据目前是放在Hadoop里的，每天的倒排索引也很大，应该也要放在Hadoop中。

如果把日志数据中每一行都当作一个文档的话，总计需要700亿个ID，超出int32的范围，占用空间太大。如果每128行日志组合为一个文档的话，有如下几个优点：
1. 5.5亿，int32 够用
2. 大大减少索引数据所占存储空间
3. 类似于块存储概念
4. 128可以最大限度利用varint压缩编码的优势

### 详细设计

#### 原始日志数据的存储格式设计

我们知道，原始日志数据是放在hadoop中的，考虑到都是文本数据，所以进行了GZ压缩存储。
这就带来一个问题，如果想读取一个GZ文件中的一行日志，就需要将整个GZ文件读出来、解压，然后读取我们需要的那一行数据。
这显然与我们需要的秒级搜索能力是不匹配的，因为一个日志文件，压缩后存储在hadoop中也是几百MB以上，甚至达到数GB大小。

我们了解到gzip压缩格式有一个比较好的特点：多个gzip格式的数据之间拼接到一起，gzip解压程序仍然能正常解压开。
我们利用这一特性来设计原始日志数据的存储格式，即：将原始的一个数百MB大小的gz文件，
按一定的大小(例如10KB)分隔为很多个这种小的gz文件，然后拼接在一起，组成一个大的gz文件存入hadoop。
这样不破坏现有的hadoop文件格式，也就不破坏基于该日志上处理的一下Map/Reduce作业程序。

详细格式请见下图：


## 参考

1. [《这就是搜素引擎-核心技术详解》](http://www.infoq.com/cn/minibooks/this-is-search-engine)
2. [Elasticsearch](https://www.elastic.co/)





